#!/bin/bash
#PBS -N coder_server_glm46
#PBS -o /dev/null
#PBS -e /dev/null
#PBS -q AISG_large
#PBS -l select=1:mem=1000gb:ngpus=8:ncpus=100:host=hopper-46
#PBS -l walltime=240:00:00

set -e

export SHARED_FS="/scratch/Projects/SPEC-SF-AISG"
export CONTAINER_NAME="yuli_vllm_glm46"
export SQSH_DIR="${SHARED_FS}/sqsh"
export IMG_NAME="vllm-openai"
export LOCAL_IMG2="${SQSH_DIR}/${IMG_NAME}-v0.11.2.sqsh"
export SQSH_FILE=$LOCAL_IMG2

export CACHE_ROOT="${SHARED_FS}/cache"
export HF_HOME="${SHARED_FS}/cache/huggingface_tmp" # defaults to non_tmp
export VLLM_CACHE_ROOT="$CACHE_ROOT/vllm_cache"
export TORCH_HOME="$CACHE_ROOT/torch_cache"
export TORCH_KERNEL_CACHE_PATH="$CACHE_ROOT/torch_cache/kernels"
# Use per-job Triton cache to avoid race conditions with multi-GPU workers
export TRITON_CACHE_DIR="$CACHE_ROOT/triton_cache_glm46_${PBS_JOBID:-$$}"
export XDG_CACHE_HOME="$CACHE_ROOT/xdg_cache"
export CUDA_CACHE_PATH="$CACHE_ROOT/cuda_cache"

export LOG_DIR="${SHARED_FS}/log/mech/server/${PBS_JOBID}"
mkdir -p "${LOG_DIR}"

exec > >(tee -a "${LOG_DIR}/pbs.log") 2>&1

# Set enroot data directory to shared filesystem to avoid home directory space issues
export ENROOT_DATA_PATH="${SHARED_FS}/.enroot/data"
mkdir -p "${ENROOT_DATA_PATH}"

# Create container if it doesn't exist
if ! enroot list | grep -q "${CONTAINER_NAME}"; then
    enroot create -n "${CONTAINER_NAME}" "${SQSH_FILE}"
fi

# Set up cache directories on shared filesystem to avoid disk space issues
mkdir -p "${HF_HOME}" "${VLLM_CACHE_ROOT}" "${TORCH_HOME}/kernels" "${TRITON_CACHE_DIR}" "${XDG_CACHE_HOME}" "${CUDA_CACHE_PATH}"

# Set CUDA_VISIBLE_DEVICES to numeric indices to fix vLLM UUID issue
if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
    num_gpus=$(nvidia-smi -L | wc -l)
    export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((num_gpus - 1)))
fi

# Build enroot --env arguments: pass all variables matching prefixes
env_args=""
prefixes=("NCCL" "CUDA" "SHARED" "HF" "WANDB" "XDG" "LOG" "CACHE" "TORCH" "TRITON" "VLLM")

for prefix in "${prefixes[@]}"; do
    while IFS= read -r var; do
        if [ -n "${!var}" ]; then
            env_args="${env_args} --env=${var}=${!var}"
        fi
    done < <(env | grep "^${prefix}_" | cut -d'=' -f1)
done

# print all values of env_args
echo "env_args: ${env_args}"

# ========================================
# GLM-4.6 Model Configuration
# ========================================
# Zhipu GLM-4.6 is a Mixture-of-Experts (MoE) model
# Using tensor parallelism 8 for single-user low-latency workload
# with expert parallelism enabled for efficient MoE distribution
MODEL_NAME="zai-org/GLM-4.6"
SERVED_MODEL_NAME="glm-4.6-coder"
PORT=8002

# GLM-4 chat template (model default will be used if empty)
# GLM-4 uses its own format, let vLLM auto-detect from tokenizer config

enroot start --root --rw \
    --mount="${HOME}:${HOME}" \
    --mount="${SHARED_FS}:${SHARED_FS}" \
    ${env_args} \
    ${CONTAINER_NAME} \
    --model ${MODEL_NAME} \
    --served-model-name ${SERVED_MODEL_NAME} \
    --tensor-parallel-size 8 \
    --enable-expert-parallel \
    --enforce-eager \
    --dtype bfloat16 \
    --kv-cache-dtype fp8 \
    --gpu-memory-utilization 0.95 \
    --port ${PORT} \
    --max-model-len 32768 \
    --max-num-seqs 8 \
    --max-num-batched-tokens 32768 \
    --enable-chunked-prefill \
    --enable-prefix-caching \
    --swap-space 32 \
    --trust-remote-code 2>&1 &

# Note: SSH reverse tunnels don't work between compute nodes on this HPC.
# Clients should connect directly using the server hostname.
# Example: export OPENAI_API_BASE=http://$(hostname):8002

echo "GLM-4.6 Coder server running on $(hostname):${PORT}"
echo "Clients should connect directly to: http://$(hostname):${PORT}"
echo "Served model name: ${SERVED_MODEL_NAME}"

# Wait for server to finish (keeps PBS job alive)
wait
