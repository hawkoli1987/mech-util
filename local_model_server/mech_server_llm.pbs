#!/bin/bash
#PBS -N llm_server_2
#PBS -o /dev/null
#PBS -e /dev/null
#PBS -q AISG_large
#PBS -l select=1:mem=1000gb:ngpus=8:ncpus=100:host=hopper-45
#PBS -l walltime=240:00:00

set -e

export SHARED_FS="/scratch/Projects/SPEC-SF-AISG"
export CONTAINER_NAME="yuli_vllm2"
export SQSH_DIR="${SHARED_FS}/sqsh"
export IMG_NAME="vllm-openai"
export LOCAL_IMG2="${SQSH_DIR}/${IMG_NAME}-v0.11.2.sqsh"
export SQSH_FILE=$LOCAL_IMG2

export CACHE_ROOT="${SHARED_FS}/cache"
export HF_HOME="${SHARED_FS}/cache/huggingface_tmp" # defaults to non_tmp
export VLLM_CACHE_ROOT="$CACHE_ROOT/vllm_cache"
export TORCH_HOME="$CACHE_ROOT/torch_cache"
export TORCH_KERNEL_CACHE_PATH="$CACHE_ROOT/torch_cache/kernels"
export TRITON_CACHE_DIR="$CACHE_ROOT/triton_cache"
export XDG_CACHE_HOME="$CACHE_ROOT/xdg_cache"
export CUDA_CACHE_PATH="$CACHE_ROOT/cuda_cache"

export LOG_DIR="${SHARED_FS}/log/mech/server/${PBS_JOBID}"
mkdir -p "${LOG_DIR}"

exec > >(tee -a "${LOG_DIR}/pbs.log") 2>&1

# Set enroot data directory to shared filesystem to avoid home directory space issues
export ENROOT_DATA_PATH="${SHARED_FS}/.enroot/data"
mkdir -p "${ENROOT_DATA_PATH}"

# Create container if it doesn't exist
if ! enroot list | grep -q "${CONTAINER_NAME}"; then
    enroot create -n "${CONTAINER_NAME}" "${SQSH_FILE}"
fi

# Set up cache directories on shared filesystem to avoid disk space issues
mkdir -p "${HF_HOME}" "${VLLM_CACHE_ROOT}" "${TORCH_HOME}/kernels" "${TRITON_CACHE_DIR}" "${XDG_CACHE_HOME}" "${CUDA_CACHE_PATH}"

# Set CUDA_VISIBLE_DEVICES to numeric indices to fix vLLM UUID issue
if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
    num_gpus=$(nvidia-smi -L | wc -l)
    export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((num_gpus - 1)))
fi

# Build enroot --env arguments: pass all variables matching prefixes
env_args=""
prefixes=("NCCL" "CUDA" "SHARED" "HF" "WANDB" "XDG" "LOG" "CACHE" "TORCH" "TRITON" "VLLM")

for prefix in "${prefixes[@]}"; do
    while IFS= read -r var; do
        if [ -n "${!var}" ]; then
            env_args="${env_args} --env=${var}=${!var}"
        fi
    done < <(env | grep "^${prefix}_" | cut -d'=' -f1)
done

# print all values of env_args
echo "env_args: ${env_args}"

MODEL_NAME="deepseek-ai/DeepSeek-V3.2"

# Chat template for DeepSeek
CHAT_TEMPLATE='{% for message in messages %}{% if message['"'"'role'"'"'] == '"'"'system'"'"' %}<|im_start|>system
{{ message['"'"'content'"'"'] }}<|im_end|>
{% elif message['"'"'role'"'"'] == '"'"'user'"'"' %}<|im_start|>user
{{ message['"'"'content'"'"'] }}<|im_end|>
{% elif message['"'"'role'"'"'] == '"'"'assistant'"'"' %}<|im_start|>assistant
{{ message['"'"'content'"'"'] }}<|im_end|>
{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant
{% endif %}'

# Speculative decoding config for DeepSeek MTP
SPECULATIVE_CONFIG='{"method": "deepseek_mtp", "num_speculative_tokens": 1}'

enroot start --root --rw \
    --mount="${HOME}:${HOME}" \
    --mount="${SHARED_FS}:${SHARED_FS}" \
    ${env_args} \
    yuli_vllm2 \
    --model ${MODEL_NAME} \
    --tensor-parallel-size 8 \
    --enable-expert-parallel \
    --dtype bfloat16 \
    --kv-cache-dtype fp8 \
    --gpu-memory-utilization 0.95 \
    --port 8001 \
    --max-model-len 8192 \
    --max-num-seqs 16 \
    --max-num-batched-tokens 8192 \
    --enable-chunked-prefill \
    --enable-prefix-caching \
    --swap-space 32 \
    --chat-template "${CHAT_TEMPLATE}" 2>&1 &

# Note: SSH reverse tunnels don't work between compute nodes on this HPC.
# Clients should connect directly using the server hostname.
# Example: export OPENAI_API_BASE=http://$(hostname):8001

echo "Servers running on $(hostname):8001"
echo "Clients should connect directly to: http://$(hostname):8001"

# Wait for both servers to finish (keeps PBS job alive)
wait
