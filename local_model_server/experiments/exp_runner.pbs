#!/bin/bash
#PBS -N glm46_exp
#PBS -o /dev/null
#PBS -e /dev/null
#PBS -q AISG_large
#PBS -l select=1:mem=1000gb:ngpus=8:ncpus=100
#PBS -l walltime=8:00:00

# ============================================================================
# GLM-4.6 Experiment Runner - Two-Phase Approach
# ============================================================================
# This script uses a two-phase approach to dynamically determine optimal
# max_num_seqs for each configuration:
#
# Phase 1: Probe run - Start server with max_num_seqs=1, extract the actual
#          max_concurrency that vLLM calculated based on available KV cache
# Phase 2: Actual run - Restart server with optimal max_num_seqs and run benchmark
#
# Configuration variables - override with qsub -v
#
# Basic config:
#   EXP_NAME: Experiment name (required)
#   ENFORCE_EAGER: "yes" or "no" (default: yes)
#   ENABLE_EP: "yes" or "no" - Expert Parallelism (default: yes)
#   MAX_MODEL_LEN: 8192, 16384, 32768, 65536, 131072 (default: 32768)
#
# Advanced config:
#   KV_CACHE_DTYPE: "fp8" or "auto" (default: fp8)
#   ENABLE_PREFIX_CACHING: "yes" or "no" (default: yes)
#   ENABLE_CHUNKED_PREFILL: "yes" or "no" (default: yes)
#
# Benchmark config:
#   NUM_SAMPLES: Number of samples to benchmark (default: 100)
#   DATASET_TYPE: "8k" or "64k" (default: 8k)
#
# Note: MTP (Multi-Token Prediction) is NOT supported for GLM-4.6 in vLLM 0.12
#       GLM-4.6 has num_nextn_predict_layers=1 but vLLM doesn't support GLM MTP
# ============================================================================

set -e

# Parse configuration with defaults
EXP_NAME="${EXP_NAME:-default}"
ENFORCE_EAGER="${ENFORCE_EAGER:-yes}"
ENABLE_EP="${ENABLE_EP:-yes}"
MAX_MODEL_LEN="${MAX_MODEL_LEN:-32768}"
KV_CACHE_DTYPE="${KV_CACHE_DTYPE:-fp8}"
ENABLE_PREFIX_CACHING="${ENABLE_PREFIX_CACHING:-yes}"
ENABLE_CHUNKED_PREFILL="${ENABLE_CHUNKED_PREFILL:-yes}"
NUM_SAMPLES="${NUM_SAMPLES:-100}"
DATASET_TYPE="${DATASET_TYPE:-8k}"
EXTRA_ARGS="${EXTRA_ARGS:-}"

# Environment setup
export SHARED_FS="/scratch/Projects/SPEC-SF-AISG"
export CONTAINER_NAME="yuli_vllm_exp_${EXP_NAME}"
# Use vLLM 0.12.0 from the new location
export SQSH_FILE="${SHARED_FS}/source_files/Mech/4-sqsh/vllm-openai-v0.12.0.sqsh"

export CACHE_ROOT="${SHARED_FS}/cache"
export HF_HOME="${SHARED_FS}/cache/huggingface_tmp"
export VLLM_CACHE_ROOT="$CACHE_ROOT/vllm_cache"
export TORCH_HOME="$CACHE_ROOT/torch_cache"
export TORCH_KERNEL_CACHE_PATH="$CACHE_ROOT/torch_cache/kernels"
export TRITON_CACHE_DIR="/tmp/triton_cache_${PBS_JOBID:-$$}"
export XDG_CACHE_HOME="$CACHE_ROOT/xdg_cache"
export CUDA_CACHE_PATH="$CACHE_ROOT/cuda_cache"

export LOG_DIR="${SHARED_FS}/log/mech/experiments/${EXP_NAME}/${PBS_JOBID}"
mkdir -p "${LOG_DIR}"

exec > >(tee -a "${LOG_DIR}/pbs.log") 2>&1

echo "========================================"
echo "Experiment: ${EXP_NAME}"
echo "PBS Job ID: ${PBS_JOBID}"
echo "Host: $(hostname)"
echo "========================================"
echo "Basic Configuration:"
echo "  ENFORCE_EAGER: ${ENFORCE_EAGER}"
echo "  ENABLE_EP: ${ENABLE_EP}"
echo "  MAX_MODEL_LEN: ${MAX_MODEL_LEN}"
echo "========================================"
echo "Advanced Configuration:"
echo "  KV_CACHE_DTYPE: ${KV_CACHE_DTYPE}"
echo "  ENABLE_PREFIX_CACHING: ${ENABLE_PREFIX_CACHING}"
echo "  ENABLE_CHUNKED_PREFILL: ${ENABLE_CHUNKED_PREFILL}"
echo "========================================"
echo "Benchmark Configuration:"
echo "  NUM_SAMPLES: ${NUM_SAMPLES}"
echo "  DATASET_TYPE: ${DATASET_TYPE}"
echo "  EXTRA_ARGS: ${EXTRA_ARGS}"
echo "========================================"

export ENROOT_DATA_PATH="${SHARED_FS}/.enroot/data"
mkdir -p "${ENROOT_DATA_PATH}"

# Create container if it doesn't exist
if ! enroot list | grep -q "${CONTAINER_NAME}"; then
    enroot create -n "${CONTAINER_NAME}" "${SQSH_FILE}"
fi

# Setup cache directories
mkdir -p "${HF_HOME}" "${VLLM_CACHE_ROOT}" "${TORCH_HOME}/kernels" "${TRITON_CACHE_DIR}" "${XDG_CACHE_HOME}" "${CUDA_CACHE_PATH}"

# Set CUDA_VISIBLE_DEVICES
if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
    num_gpus=$(nvidia-smi -L | wc -l)
    export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((num_gpus - 1)))
fi

# Build environment arguments for enroot
env_args=""
prefixes=("NCCL" "CUDA" "SHARED" "HF" "WANDB" "XDG" "LOG" "CACHE" "TORCH" "TRITON" "VLLM")
for prefix in "${prefixes[@]}"; do
    while IFS= read -r var; do
        if [ -n "${!var}" ]; then
            env_args="${env_args} --env=${var}=${!var}"
        fi
    done < <(env | grep "^${prefix}_" | cut -d'=' -f1)
done

# Model configuration
MODEL_NAME="zai-org/GLM-4.6"
SERVED_MODEL_NAME="glm-4.6-exp"
PORT=8003

# ============================================================================
# Build vLLM command arguments
# ============================================================================
VLLM_ARGS="--model ${MODEL_NAME}"
VLLM_ARGS="${VLLM_ARGS} --served-model-name ${SERVED_MODEL_NAME}"
VLLM_ARGS="${VLLM_ARGS} --tensor-parallel-size 8"
VLLM_ARGS="${VLLM_ARGS} --dtype bfloat16"
VLLM_ARGS="${VLLM_ARGS} --gpu-memory-utilization 0.95"
VLLM_ARGS="${VLLM_ARGS} --port ${PORT}"
VLLM_ARGS="${VLLM_ARGS} --max-model-len ${MAX_MODEL_LEN}"
VLLM_ARGS="${VLLM_ARGS} --swap-space 32"
VLLM_ARGS="${VLLM_ARGS} --trust-remote-code"

# KV Cache dtype
VLLM_ARGS="${VLLM_ARGS} --kv-cache-dtype ${KV_CACHE_DTYPE}"

# Note: max_num_seqs and max_num_batched_tokens are NOT set
# vLLM will auto-calculate optimal values based on available memory

# Eager mode (disables torch.compile and CUDA graphs)
if [ "${ENFORCE_EAGER}" = "yes" ]; then
    VLLM_ARGS="${VLLM_ARGS} --enforce-eager"
fi

# Expert Parallelism for MoE
if [ "${ENABLE_EP}" = "yes" ]; then
    VLLM_ARGS="${VLLM_ARGS} --enable-expert-parallel"
fi

# Prefix caching
if [ "${ENABLE_PREFIX_CACHING}" = "yes" ]; then
    VLLM_ARGS="${VLLM_ARGS} --enable-prefix-caching"
fi

# Chunked prefill
# Note: vLLM V1 engine defaults to chunked prefill enabled
# We need to explicitly disable it with --no-enable-chunked-prefill
if [ "${ENABLE_CHUNKED_PREFILL}" = "yes" ]; then
    VLLM_ARGS="${VLLM_ARGS} --enable-chunked-prefill"
else
    VLLM_ARGS="${VLLM_ARGS} --no-enable-chunked-prefill"
fi

# NOTE: MTP (Multi-Token Prediction) speculative decoding is NOT supported
# GLM-4.6 has num_nextn_predict_layers=1 but vLLM 0.12 doesn't have GLM MTP support
# ngram-based speculation doesn't use the model's MTP heads (only DeepSeek MTP is supported)

# Additional arguments
VLLM_ARGS="${VLLM_ARGS} ${EXTRA_ARGS}"

# ============================================================================
# Helper function: Start vLLM server with given max_num_seqs
# ============================================================================
start_vllm_server() {
    local max_seqs=$1
    local phase=$2
    
    local args="${VLLM_ARGS}"
    if [ -n "${max_seqs}" ] && [ "${max_seqs}" != "0" ]; then
        args="${args} --max-num-seqs ${max_seqs}"
    fi
    
    echo ""
    echo "[${phase}] Starting vLLM with arguments:"
    echo "${args}" | tr ' ' '\n' | grep -v '^$' | sed 's/^/  /'
    echo ""
    
    enroot start --root --rw \
        --mount="${HOME}:${HOME}" \
        --mount="${SHARED_FS}:${SHARED_FS}" \
        ${env_args} \
        ${CONTAINER_NAME} \
        ${args} 2>&1 &
    
    SERVER_PID=$!
    
    # Wait for server to be ready (max 30 minutes for compilation)
    echo "[${phase}] Waiting for server to start (max 30 minutes)..."
    local MAX_WAIT=1800
    local WAIT_INTERVAL=30
    local ELAPSED=0
    
    while [ $ELAPSED -lt $MAX_WAIT ]; do
        if curl -s "http://$(hostname):${PORT}/health" > /dev/null 2>&1; then
            echo "[${phase}] Server is ready after $ELAPSED seconds"
            return 0
        fi
        
        # Check if server process died
        if ! kill -0 $SERVER_PID 2>/dev/null; then
            echo "[${phase}] ERROR: Server process died unexpectedly"
            return 1
        fi
        
        sleep $WAIT_INTERVAL
        ELAPSED=$((ELAPSED + WAIT_INTERVAL))
        echo "  Still waiting... ($ELAPSED seconds elapsed)"
    done
    
    echo "[${phase}] ERROR: Server failed to start after $MAX_WAIT seconds"
    kill $SERVER_PID 2>/dev/null || true
    return 1
}

# ============================================================================
# Helper function: Extract max_num_seqs from vLLM startup logs
# ============================================================================
# vLLM automatically calculates and logs the maximum concurrency during startup:
#   "Maximum concurrency for 32,768 tokens per request: 61.44x"
# We extract this value instead of manually calculating from GPU blocks.
# ============================================================================
extract_max_concurrency() {
    local log_file="${LOG_DIR}/pbs.log"
    
    # vLLM logs: "Maximum concurrency for X tokens per request: Y.YYx"
    # We extract the decimal value Y.YY and floor it to get max_num_seqs
    local max_concurrency=$(grep -oP 'Maximum concurrency for [0-9,]+ tokens per request: \K[0-9.]+' "${log_file}" 2>/dev/null | tail -1)
    
    if [ -n "${max_concurrency}" ]; then
        # Floor the decimal value to get integer max_num_seqs
        local max_seqs=$(python3 -c "import math; print(int(math.floor(${max_concurrency})))")
        
        # Ensure at least 1 sequence
        if [ "${max_seqs}" -lt 1 ]; then
            max_seqs=1
        fi
        
        echo "  vLLM reported: Maximum concurrency = ${max_concurrency}x"
        echo "  Using max_num_seqs: ${max_seqs}"
        echo "${max_seqs}"
        return 0
    fi
    
    # Fallback: check for GPU blocks if Maximum concurrency not found
    local gpu_blocks=$(grep -oP '# GPU blocks: \K\d+' "${log_file}" 2>/dev/null | tail -1)
    if [ -n "${gpu_blocks}" ]; then
        echo "  WARNING: Maximum concurrency not found, falling back to GPU blocks calculation"
        local block_size=16
        local blocks_per_seq=$(( (MAX_MODEL_LEN + block_size - 1) / block_size ))
        local max_seqs=$(( gpu_blocks / blocks_per_seq ))
        if [ "${max_seqs}" -lt 1 ]; then max_seqs=1; fi
        echo "  GPU blocks: ${gpu_blocks}, calculated max_num_seqs: ${max_seqs}"
        echo "${max_seqs}"
        return 0
    fi
    
    echo "  ERROR: Could not extract max concurrency from vLLM logs"
    echo "  Check ${log_file} for vLLM startup output"
    echo "256"  # Safe default
    return 1
}

# ============================================================================
# PHASE 1: Probe run to discover optimal max_num_seqs
# ============================================================================
echo ""
echo "========================================"
echo "PHASE 1: Probe run to discover max_num_seqs"
echo "========================================"

# Start with max_num_seqs=1 to minimize memory overhead during probe
start_vllm_server 1 "PROBE"
if [ $? -ne 0 ]; then
    echo "PHASE 1 FAILED: Could not start server for probe"
    exit 1
fi

echo ""
echo "Extracting optimal max_num_seqs from probe run..."
OPTIMAL_MAX_SEQS=$(extract_max_concurrency | tail -1)
echo "Optimal max_num_seqs: ${OPTIMAL_MAX_SEQS}"

# Stop the probe server
echo "Stopping probe server..."
kill $SERVER_PID 2>/dev/null || true
wait $SERVER_PID 2>/dev/null || true
sleep 5

# ============================================================================
# PHASE 2: Actual run with optimal max_num_seqs
# ============================================================================
echo ""
echo "========================================"
echo "PHASE 2: Actual run with max_num_seqs=${OPTIMAL_MAX_SEQS}"
echo "========================================"

start_vllm_server "${OPTIMAL_MAX_SEQS}" "ACTUAL"
if [ $? -ne 0 ]; then
    echo "PHASE 2 FAILED: Could not start server for actual run"
    exit 1
fi

# Get server info
echo ""
echo "Server running on $(hostname):${PORT}"
curl -s "http://$(hostname):${PORT}/v1/models" | python3 -c "
import sys, json
data = json.load(sys.stdin)
if data.get('data'):
    m = data['data'][0]
    print(f\"  Model: {m.get('id')}\")
    print(f\"  Max model len: {m.get('max_model_len')}\")
" 2>/dev/null || echo "  (Could not retrieve model info)"
echo "  Max num seqs: ${OPTIMAL_MAX_SEQS}"
echo ""

# ============================================================================
# Run benchmark
# ============================================================================
BENCHMARK_DIR="${SHARED_FS}/source_files/Mech/mech-util/local_model_server"
DATASET_DIR="${SHARED_FS}/source_files/Mech/storage/samples/glm46_benchmark"

# Select dataset
if [ "${DATASET_TYPE}" = "64k" ]; then
    DATASET_FILE="${DATASET_DIR}/dataset_64k.json"
else
    DATASET_FILE="${DATASET_DIR}/dataset_8k.json"
fi

echo "Running benchmark with ${NUM_SAMPLES} samples from ${DATASET_TYPE} dataset..."

python3 ${BENCHMARK_DIR}/benchmark_runner.py \
    --base-url "http://$(hostname):${PORT}" \
    --model "${SERVED_MODEL_NAME}" \
    --dataset "${DATASET_FILE}" \
    --samples ${NUM_SAMPLES} \
    --config-name "${EXP_NAME}" \
    --output "${LOG_DIR}/results_${DATASET_TYPE}.json"

# If max_model_len is large enough, also test with 64k dataset
if [ "${DATASET_TYPE}" = "8k" ] && [ "${MAX_MODEL_LEN}" -ge 131072 ]; then
    echo ""
    echo "Also running benchmark with 64k dataset..."
    python3 ${BENCHMARK_DIR}/benchmark_runner.py \
        --base-url "http://$(hostname):${PORT}" \
        --model "${SERVED_MODEL_NAME}" \
        --dataset "${DATASET_DIR}/dataset_64k.json" \
        --samples $((NUM_SAMPLES / 2)) \
        --config-name "${EXP_NAME}_64k" \
        --output "${LOG_DIR}/results_64k.json"
fi

echo ""
echo "========================================"
echo "Benchmark complete!"
echo "Results saved to: ${LOG_DIR}"
echo "========================================"

# Collect final server stats
echo ""
echo "Final server metrics:"
curl -s "http://$(hostname):${PORT}/metrics" 2>/dev/null | grep -E "(vllm_|gpu_)" | head -20 || echo "  (Metrics not available)"

# Keep server running for additional testing if needed
# Uncomment the following line to end job after benchmark
# kill $SERVER_PID 2>/dev/null || true; exit 0

echo ""
echo "Server still running for manual testing. Job will end when walltime expires."
echo "Connect to: http://$(hostname):${PORT}"

# Wait for server
wait $SERVER_PID
